{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYSKuxdrMt1K"
   },
   "source": [
    "# AI Challenge Notebook\n",
    "\n",
    "### Team name: France-INSA/ENSEEIHT/VALDOM-UPENDO\n",
    "### Team Members: \n",
    "* GHOMSI KONGA Serge\n",
    "* KEITA Alfousseyni\n",
    "* RIDA Moumni\n",
    "* SANOU Désiré\n",
    "* WAFFA PAGOU Brondon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBEp8lKZOul4"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "For this project, we tested several models including BERT, Logistic Regression, Embedding,lstm-gru-cnn-glove and SVC. With individual models, we didn't get the accuracy we were excepting. Therefore, we chosed those giving the best accuracies ( i.e Bert, SVC and lstm-gru-cnn-glove), and performed a maojority voting on them. <br> BERT had the best accuracy among the three models, so we gave it the priority in case all the three predictions are different. <br>\n",
    "In this notebook are the implementations of the three models including preporcessing and the majority voting code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM5FjG4_R4lv"
   },
   "source": [
    "#  Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnDmzFsYW2CN"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhWyx-3qW4mt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fnR2QuXSV_V"
   },
   "outputs": [],
   "source": [
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import re \n",
    "import collections\n",
    "import itertools\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import plotly.offline as pof\n",
    "import plotly.graph_objects as go\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sklearn.metrics as smet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import classification_report\n",
    "sb.set_style(\"whitegrid\")\n",
    "import sklearn.model_selection as sms\n",
    "!pip install git+https://github.com/abhishekkrthakur/tez.git\n",
    "!pip install transformers==3.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWoCwux8Sx7a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dplnBupSDLx"
   },
   "source": [
    "# BERT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03JvjJSddaBE"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFjYWJ3ydm8W"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtGPNVFadjlM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = \"/content/drive/MyDrive/AI_DEFI/defi-ia-insa-toulouse\"\n",
    "train_df = pd.read_json(DATA_PATH+\"/train.json\")\n",
    "test_df = pd.read_json(DATA_PATH+\"/test.json\")\n",
    "train_label = pd.read_csv(DATA_PATH+\"/train_label.csv\")\n",
    "categories_string = pd.read_csv(DATA_PATH+\"/categories_string.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8vPpdkkd1s-"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxs_78sGd4Hq"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEkQsNC_d-Z-"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from string import digits\n",
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "\n",
    "digits_list = digits\n",
    "\n",
    "class CleanText:\n",
    "    def __init__(self):\n",
    "        french_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        self.stopwords = [self.remove_accent(sw) for sw in french_stopwords]\n",
    "\n",
    "        self.stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    @staticmethod\n",
    "    def remove_html_code(txt):\n",
    "        txt = BeautifulSoup(txt, \"html.parser\", from_encoding='utf-8').get_text()\n",
    "        return txt\n",
    "    @staticmethod\n",
    "    def convert_text_to_lower_case(txt):\n",
    "        return txt.lower()\n",
    "    @staticmethod\n",
    "    def remove_accent(txt):\n",
    "        return unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    @staticmethod\n",
    "    def remove_non_letters(txt):\n",
    "        return re.sub('[^a-z_]', ' ', txt)\n",
    "    def remove_stopwords(self, txt):\n",
    "        return [w for w in txt.split() if (w not in self.stopwords)]\n",
    "    def get_stem(self, tokens):\n",
    "        return [self.stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKp_dR04eEDx"
   },
   "outputs": [],
   "source": [
    "cleaner = CleanText()\n",
    "def apply_all_transformation(txt):\n",
    "    cleaned_txt = cleaner.remove_html_code(txt)\n",
    "    cleaned_txt = cleaner.convert_text_to_lower_case(cleaned_txt)\n",
    "    cleaned_txt = cleaner.remove_accent(cleaned_txt)\n",
    "    cleaned_txt = cleaner.remove_non_letters(cleaned_txt)\n",
    "    cleaned_txt = cleaner.remove_stopwords(cleaned_txt)\n",
    "    cleaned_txt = cleaner.get_stem(cleaned_txt)\n",
    "    return cleaned_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HOD-sM6eHwF"
   },
   "outputs": [],
   "source": [
    "def clean_df_column(dataset, column, cleaned_column):\n",
    "    dirty_column = dataset[str(column)]\n",
    "    clean = [\" \".join(apply_all_transformation(x)) for x in tqdm(dirty_column)]\n",
    "    dataset[str(cleaned_column)] = clean\n",
    "clean_df_column(train_df,'description','description_cleaned')\n",
    "clean_df_column(test_df,'description','description_cleaned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpUnax6xef5o"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ci3v-Kc2eSMp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tez\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class BERTDataset:\n",
    "    def __init__(self, text, target):\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.max_len = 64\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.target[item], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "class BERTBaseUncased(tez.Model):\n",
    "    def __init__(self, num_train_steps, num_classes):\n",
    "        super().__init__()\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, num_classes)\n",
    "\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.step_scheduler_after = \"batch\"\n",
    "\n",
    "    def fetch_optimizer(self):\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        opt = AdamW(optimizer_parameters, lr=3e-5)\n",
    "        return opt\n",
    "\n",
    "    def fetch_scheduler(self):\n",
    "        sch = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=0, num_training_steps=self.num_train_steps\n",
    "        )\n",
    "        return sch\n",
    "\n",
    "    def loss(self, outputs, targets):\n",
    "        if targets is None:\n",
    "            return None\n",
    "        return nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "    def monitor_metrics(self, outputs, targets):\n",
    "        if targets is None:\n",
    "            return {}\n",
    "        outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        return {\"accuracy\": accuracy}\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, targets=None):\n",
    "        _, o_2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        b_o = self.bert_drop(o_2)\n",
    "        output = self.out(b_o)\n",
    "        loss = self.loss(output, targets)\n",
    "        acc = self.monitor_metrics(output, targets)\n",
    "        return output, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MourknPce_Z7"
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "category = lbl_enc.fit_transform(train_label.Category.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmZgLybNft7r"
   },
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeLOXBJAfQI-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df.description_cleaned.values,train_label.Category.values, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJXCd-OGfxd7"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa3kcVk3fWKz"
   },
   "outputs": [],
   "source": [
    "train_dataset = BERTDataset(text=X_train, target=y_train)\n",
    "valid_dataset = BERTDataset(text=X_test, target=y_test)\n",
    "n_train_steps = int(len(X_train) / 32 * 10)\n",
    "model = BERTBaseUncased(num_train_steps=n_train_steps, num_classes=train_label.Category.nunique())\n",
    "\n",
    "tb_logger = tez.callbacks.TensorBoardLogger(log_dir=\".logs/\")\n",
    "es = tez.callbacks.EarlyStopping(monitor=\"valid_loss\", model_path=\"model.bin\")\n",
    "model.fit(\n",
    "        train_dataset,\n",
    "        valid_dataset=valid_dataset,\n",
    "        train_bs=32,\n",
    "        device=\"cuda\",\n",
    "        epochs=3,\n",
    "        callbacks=[tb_logger, es],\n",
    "        fp16=True,\n",
    "    )\n",
    "model.save(\"model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhZq4d_of0G_"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8GMTMYSfsza"
   },
   "outputs": [],
   "source": [
    "test_dataset = BERTDataset(text=X_test, target= np.full((len(X_test), ), 0))\n",
    "pred = model.predict(test_dataset, device=\"cuda\")\n",
    "\n",
    "res = []\n",
    "for p in pred:\n",
    "  res.append(p)\n",
    "results = [torch.argmax(torch.from_numpy(elem), axis=1).numpy() for elem in res]\n",
    "predict_test = []\n",
    "for elem in results:\n",
    "  for e in elem:\n",
    "    predict_test.append(e)\n",
    "\n",
    "print(classification_report(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajgfrx-_gvYW"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjX9oBZcgxHM"
   },
   "outputs": [],
   "source": [
    "tar =  np.full((len(test_df.description_cleaned.values), ), 0)\n",
    "test_dataset = BERTDataset(text=test_df.description_cleaned.values, target= tar)\n",
    "pred = model.predict(test_dataset, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msNxwKi7hTRG"
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for p in pred:\n",
    "  res.append(p)\n",
    "\n",
    "results_pred = [torch.argmax(torch.from_numpy(elem), axis=1).numpy() for elem in res]\n",
    "\n",
    "predict_csv = []\n",
    "for elem in results_pred:\n",
    "  for e in elem:\n",
    "    predict_csv.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPT9MGkdiHYP"
   },
   "outputs": [],
   "source": [
    "test_df[\"Category\"] = predict_csv\n",
    "bert_pred = test_df[[\"Id\",\"Category\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msqXrSOfSW1n"
   },
   "source": [
    "# SVC Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvPPebPdkZZR"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o58vvjN8Scyi"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "test =np.append(train_df[\"description_cleaned\"].values,test_df[\"description_cleaned\"].values)\n",
    "\n",
    "TF_IDF = CountVectorizer(ngram_range=(1,2))\n",
    "TF_IDF.fit(test)\n",
    "train_df_TFIDF = TF_IDF.transform(train_df[\"description_cleaned\"].values)\n",
    "test_df_TFIDF = TF_IDF.transform(test_df[\"description_cleaned\"].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oyG8hi7ki3h"
   },
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwuIdYg-khvQ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df_TFIDF, train_label.Category.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDTg9EW8kqaB"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r61IO9B4ksf5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t15h8sy8k25A"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhjh0a0YkwSA"
   },
   "outputs": [],
   "source": [
    "y_test_pred =svc.predict(X_test)\n",
    "print(classification_report(y_test, y_test_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoEtbUGs3QpP"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWhlbnfjlGS8"
   },
   "outputs": [],
   "source": [
    "test_df[\"Category\"] = svc.predict(test_df_TFIDF)\n",
    "svc_pred = test_df[[\"Id\",\"Category\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWCAFWiSSc9r"
   },
   "source": [
    "# LSTM-GRU-CNN-Glove Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77hdBtCRRi5V"
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVAYqOL_SqVR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "np.random.seed(13)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "\n",
    "import logging\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znS4f3AARlu1"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gx7Hp-4NRz71"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/content/drive/MyDrive/AI_DEFI/defi-ia-insa-toulouse\"\n",
    "data_df = pd.read_json(DATA_PATH+\"/train.json\")\n",
    "data_df = data_df.set_index('Id',drop=False)\n",
    "data_df.index.name = None\n",
    "\n",
    "test_df = pd.read_json(DATA_PATH+\"/test.json\")\n",
    "test_df = test_df.set_index('Id',drop=False)\n",
    "test_df.index.name = None\n",
    "\n",
    "data_label = pd.read_csv(DATA_PATH+\"/train_label.csv\")\n",
    "categories_string = pd.read_csv(DATA_PATH+\"/categories_string.csv\")\n",
    "\n",
    "! mkdir data_glove\n",
    "! wget -P data_glove https://drive.google.com/file/d/1QwQs-kS1HtH_QZ_k5Mf-chvLBHDTOysR/view?usp=sharing\n",
    "#embedding_path = DATA_PATH+\"/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n",
    "embedding_path = data_glove+\"/glove840b300dtxt/glove.840B.300d.txt\"\n",
    "\n",
    "\n",
    "embed_size = 300\n",
    "max_features = 130000\n",
    "max_len = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqPeKCtbR_Go"
   },
   "source": [
    "\n",
    "## Text Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7VL2lGfSZWz"
   },
   "outputs": [],
   "source": [
    "X_train_test, X_valid, Y_train_test, Y_valid = train_test_split(data_df, data_label, test_size=0.2, random_state=13)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train_test, Y_train_test, test_size=0.2, random_state=13)\n",
    "\n",
    "\n",
    "raw_text_train = X_train[\"description\"].str.lower()\n",
    "raw_text_valid = X_valid[\"description\"].str.lower()\n",
    "raw_text_test = X_test[\"description\"].str.lower()\n",
    "raw_text_df_test = test_df[\"description\"].str.lower()\n",
    "\n",
    "tk = Tokenizer(num_words = max_features, lower = True)\n",
    "tk.fit_on_texts(raw_text_train)\n",
    "X_train[\"description_seq\"] = tk.texts_to_sequences(raw_text_train)\n",
    "X_valid[\"description_seq\"] = tk.texts_to_sequences(raw_text_valid)\n",
    "X_test[\"description_seq\"] = tk.texts_to_sequences(raw_text_test)\n",
    "test_df[\"description_seq\"] = tk.texts_to_sequences(raw_text_df_test)\n",
    "\n",
    "X_train = pad_sequences(X_train.description_seq, maxlen = max_len)\n",
    "X_valid = pad_sequences(X_valid.description_seq, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test.description_seq, maxlen = max_len)\n",
    "test = pad_sequences(test_df.description_seq, maxlen = max_len)\n",
    "\n",
    "\n",
    "Y_train = Y_train.Category.values\n",
    "Y_valid = Y_valid.Category.values\n",
    "Y_test = Y_test.Category.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGjEb5CZS4TI"
   },
   "source": [
    "## Utilities functions and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON-ezoFPS7MQ"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            \n",
    "            f1_score_macro = f1_score(self.y_val, y_pred, average=\"macro\")\n",
    "            print(\"\\n f1_score_macro - epoch: {:d} - f1_score_macro: {:.6f}\".format(epoch+1, f1_score_macro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAW666-hSeLX"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvktbulySwLn"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohbxQ848S0Cz"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n",
    "\n",
    "file_path = \"best_model.hdf5\"\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                              save_best_only = True, mode = \"min\")\n",
    "ra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    "\n",
    "def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0,epochs=3):\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    \n",
    "    y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "    x = Dense(28, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, Y_train, batch_size = 128, epochs = epochs, validation_data = (X_valid, Y_valid), \n",
    "                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ks7ZMeJFTYoH"
   },
   "outputs": [],
   "source": [
    "model = build_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.2,epochs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqzSHwsNUExi"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoQ65Iy6Tj5w"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score,accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Print accuracy, f1, precision, and recall scores\n",
    "print(\"accuracy :\", accuracy_score(Y_test, y_pred))\n",
    "print(\"precision :\", precision_score(Y_test, y_pred , average=\"macro\"))\n",
    "print(\"recall :\", recall_score(Y_test, y_pred , average=\"macro\"))\n",
    "print(\"f1_score :\", f1_score(Y_test, y_pred , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RumMLnSWUHA3"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZiuBaq-Ub4j"
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(test)\n",
    "lstm_gru_cnn_glove_pred = np.argmax(y_pred_test , axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GidNMmW2SqfW"
   },
   "source": [
    "# Final Majority Voting Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42gLDfTNVRHm"
   },
   "source": [
    "### Function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGHzXv9UT_Hz"
   },
   "outputs": [],
   "source": [
    "def GetKey(dictA, val):\n",
    "    for key, value in dictA.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return \"key doesn't exist\"\n",
    "    \n",
    "def make_final_prediction(prior_prediction, prediction2, prediction3):\n",
    "    predictions = []\n",
    "    for prior_pred, pred2, pred3 in zip(prior_prediction, prediction2, prediction3):\n",
    "        preds=[prior_pred, pred2, pred3]\n",
    "        pred_occurrence = Counter(preds)\n",
    "        if len(pred_occurrence) < len(preds):\n",
    "            val=dict(pred_occurrence).values()\n",
    "            majoritary_vote = max( val )\n",
    "            prediction= GetKey(dict(pred_occurrence),majoritary_vote)\n",
    "        else:\n",
    "            prediction = prior_pred\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koXgAdR0VbLP"
   },
   "source": [
    "### Function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34LcXjcxUGTX"
   },
   "outputs": [],
   "source": [
    "#prior prediction is bert_pred\n",
    "predictions=make_final_prediction(bert_pred[\"Category\"], svc_pred[\"Category\"] , lstm_gru_cnn_glove_pred)\n",
    "\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5FzZCLgULif"
   },
   "outputs": [],
   "source": [
    "test_df[\"Category\"] = predictions\n",
    "predictions_csv = test_df[[\"Id\",\"Category\"]]\n",
    "predictions_csv.to_csv(\"data/predictions/final_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5hDQgcgTASO"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "On kaggle, the final accuracies we had were:\n",
    "* Public score:  0.78049\n",
    "* Private score: 0.78013\n",
    "\n",
    "Based on thoses results we are can conclude that our final model, wasn't overfitted to the test data.\n",
    "This challenge helped us explore data preprocessing technics, different models and have a better knowlegde of their algorithms."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "42gLDfTNVRHm"
   ],
   "name": "NoteBook_DEFI_IA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
